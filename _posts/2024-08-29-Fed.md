---
title: "联邦学习"
date: 2024-08-29 10:00:00 +0800
categories: [FL, 科研灌水]
tags: [FL, jekyll, tutorial]
---



[TOC]









# FedIOD



### 方法概述

**问题描述**  
在联邦学习场景中，有$K$个本地节点，每个节点持有一个带标签的数据集 $\{\domx'_k, \domy'_k\}$，包括输入图像空间 $\domx'$ 和标签空间 $\domy'$。我们提出的FedIOD方法包含两个阶段。首先，在每个本地节点上训练本地模型 $T_k$。然后，将这些本地模型固定，使用生成模型 $G$ 生成伪数据来进行知识迁移。在这种一对多的知识迁移中，中央任务模型 $S$ 作为学生模型，与本地模型 $T_k$（教师模型）进行知识传递。

**输入空间的知识蒸馏**  
FedIOD中的第一阶段是输入空间的知识蒸馏。生成模型 $G$ 从随机噪声 $w$ 生成伪数据 $x$，这些伪数据用于知识转移。为了保证生成数据的真实性，我们在每个本地节点使用一个判别器 $D_k$ 来训练生成模型 $G$。生成模型 $G$ 通过使 $D_k$ 无法区分生成的伪数据与真实数据来进行训练。为了确保生成数据的语义清晰，我们最大化生成数据在每个本地模型上的预测信心。此外，为了保留每个本地模型的独特知识，我们利用Jensen-Shannon散度来最大化本地模型输出的多样性。

**输出空间的知识蒸馏**  
第二阶段是输出空间的知识蒸馏。在这个阶段，中央模型 $S$ 通过最小化其输出与本地模型输出的加权平均之间的KL散度来进行训练。由于本地数据分布的异质性，传统的输出加权方法可能效果不佳。FedIOD通过计算每个本地模型在生成数据上的重要性权重，来调整这些加权，并优化中央模型 $S$ 以更好地匹配本地模型的集成知识。生成模型 $G$ 则用于生成对中央模型具有挑战性的输入，以促进知识的迁移。

**总体损失函数**  
最终，FedIOD的总体损失函数结合了生成对抗损失、输入一致性损失、输入多样性损失以及输出模仿损失。这个损失函数的优化过程旨在通过生成具有挑战性的伪数据，增强中央模型对本地模型知识的模仿能力，从而改进模型的隐私保护和性能。







# DFDR













# Selective-FL





文章